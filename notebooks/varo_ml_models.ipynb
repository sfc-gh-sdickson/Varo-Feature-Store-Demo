{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varo ML Models - Model Registry\n",
    "\n",
    "This notebook trains ML models for the Varo Intelligence Agent:\n",
    "- **Transaction Fraud Detection** - Classify transactions as fraud or legitimate\n",
    "- **Cash Advance Eligibility** - Predict advance repayment success\n",
    "- **Customer Lifetime Value** - Predict customer LTV\n",
    "\n",
    "All models are registered to Snowflake Model Registry and can be added as tools to the Intelligence Agent.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Required Packages** (configured automatically):\n",
    "- `snowflake-ml-python`\n",
    "- `scikit-learn`\n",
    "\n",
    "**Database Context:**\n",
    "- **Database:** VARO_INTELLIGENCE  \n",
    "- **Schema:** ANALYTICS  \n",
    "- **Warehouse:** VARO_FEATURE_WH\n",
    "\n",
    "**Note:** This notebook uses Snowflake Model Registry. Ensure you have appropriate permissions to create and register models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Snowpark\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "# Import Snowpark ML\n",
    "from snowflake.ml.modeling.preprocessing import StandardScaler, OneHotEncoder\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.linear_model import LogisticRegression\n",
    "from snowflake.ml.modeling.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from snowflake.ml.modeling.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "print(\"âœ… Packages imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Snowflake\n",
    "\n",
    "Get active session and set context to Varo database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Store Registration\n",
    "\n",
    "Register our Dynamic Tables with Snowflake's native Feature Store API so they appear in the AI/ML UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Feature Store API\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore, \n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "\n",
    "# Create/Connect to Feature Store\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=\"VARO_INTELLIGENCE\",\n",
    "    name=\"FEATURE_STORE\",  # This is our schema name\n",
    "    default_warehouse=\"VARO_FEATURE_WH\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "print(\"âœ… Feature Store connected\")\n",
    "print(f\"   Location: VARO_INTELLIGENCE.FEATURE_STORE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Customer Entity\n",
    "customer_entity = Entity(\n",
    "    name=\"CUSTOMER\",\n",
    "    join_keys=[\"customer_id\"],\n",
    "    desc=\"Varo bank customer entity\"\n",
    ")\n",
    "\n",
    "fs.register_entity(customer_entity)\n",
    "print(\"âœ… CUSTOMER entity registered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Dynamic Tables as Feature Views\n",
    "\n",
    "# 1. Customer Profile Features\n",
    "customer_profile_fv = FeatureView(\n",
    "    name=\"CUSTOMER_PROFILE_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=session.table(\"VARO_INTELLIGENCE.FEATURE_STORE.CUSTOMER_PROFILE_FEATURES\"),\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    desc=\"Customer demographic, account, and direct deposit features\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=customer_profile_fv, version=\"v1\")\n",
    "print(\"âœ… CUSTOMER_PROFILE_FEATURES registered\")\n",
    "\n",
    "# 2. Transaction Pattern Features\n",
    "transaction_pattern_fv = FeatureView(\n",
    "    name=\"TRANSACTION_PATTERN_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=session.table(\"VARO_INTELLIGENCE.FEATURE_STORE.TRANSACTION_PATTERN_FEATURES\"),\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    desc=\"Transaction patterns, velocity, and spending behavior features\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=transaction_pattern_fv, version=\"v1\")\n",
    "print(\"âœ… TRANSACTION_PATTERN_FEATURES registered\")\n",
    "\n",
    "# 3. Advance Risk Features\n",
    "advance_risk_fv = FeatureView(\n",
    "    name=\"ADVANCE_RISK_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=session.table(\"VARO_INTELLIGENCE.FEATURE_STORE.ADVANCE_RISK_FEATURES\"),\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    desc=\"Cash advance history, repayment behavior, and risk scoring features\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=advance_risk_fv, version=\"v1\")\n",
    "print(\"âœ… ADVANCE_RISK_FEATURES registered\")\n",
    "\n",
    "# 4. Fraud Detection Features\n",
    "fraud_detection_fv = FeatureView(\n",
    "    name=\"FRAUD_DETECTION_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=session.table(\"VARO_INTELLIGENCE.FEATURE_STORE.FRAUD_DETECTION_FEATURES\"),\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    desc=\"Real-time fraud indicators and anomaly detection features\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=fraud_detection_fv, version=\"v1\")\n",
    "print(\"âœ… FRAUD_DETECTION_FEATURES registered\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All Feature Views registered! Check AI/ML > Features in Snowflake UI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "# Set context\n",
    "session.use_database('VARO_INTELLIGENCE')\n",
    "session.use_schema('ANALYTICS')\n",
    "session.use_warehouse('VARO_FEATURE_WH')\n",
    "\n",
    "print(f\"âœ… Connected - Role: {session.get_current_role()}\")\n",
    "print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"   Database.Schema: {session.get_fully_qualified_current_schema()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODEL 1: Transaction Fraud Detection\n",
    "\n",
    "Classify transactions as fraudulent or legitimate using customer and transaction features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Fraud Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transaction data with customer features for fraud detection\n",
    "fraud_df = session.sql(\"\"\"\n",
    "SELECT\n",
    "    t.transaction_id,\n",
    "    t.customer_id,\n",
    "    t.amount::FLOAT AS amount,\n",
    "    t.merchant_category,\n",
    "    t.transaction_type,\n",
    "    t.is_international::BOOLEAN AS is_international,\n",
    "    c.credit_score::FLOAT AS credit_score,\n",
    "    c.risk_tier,\n",
    "    COALESCE(a.current_balance, 0)::BIGINT AS account_balance,\n",
    "    -- Target: Is fraud (based on fraud_score threshold)\n",
    "    (t.fraud_score > 0.7)::BOOLEAN AS is_fraud\n",
    "FROM RAW.TRANSACTIONS t\n",
    "LEFT JOIN RAW.CUSTOMERS c ON t.customer_id = c.customer_id\n",
    "LEFT JOIN RAW.ACCOUNTS a ON t.account_id = a.account_id\n",
    "WHERE t.transaction_date >= DATEADD('month', -6, CURRENT_DATE())\n",
    "  AND t.amount > 10\n",
    "LIMIT 10000\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Fraud detection data: {fraud_df.count()} transactions\")\n",
    "fraud_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Fraud Classification Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80/20)\n",
    "train_fraud, test_fraud = fraud_df.random_split([0.8, 0.2], seed=42)\n",
    "\n",
    "# Drop ID columns\n",
    "train_fraud = train_fraud.drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\")\n",
    "test_fraud = test_fraud.drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\")\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "train_fraud = train_fraud.fillna({\"ACCOUNT_BALANCE\": 0, \"CREDIT_SCORE\": 650.0})\n",
    "test_fraud = test_fraud.fillna({\"ACCOUNT_BALANCE\": 0, \"CREDIT_SCORE\": 650.0})\n",
    "\n",
    "# Create pipeline with preprocessing and classification\n",
    "fraud_pipeline = Pipeline([\n",
    "    (\"Encoder\", OneHotEncoder(\n",
    "        input_cols=[\"MERCHANT_CATEGORY\", \"TRANSACTION_TYPE\", \"RISK_TIER\"],\n",
    "        output_cols=[\"MERCHANT_CATEGORY_ENC\", \"TRANSACTION_TYPE_ENC\", \"RISK_TIER_ENC\"],\n",
    "        drop_input_cols=True,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    (\"Scaler\", StandardScaler(\n",
    "        input_cols=[\"AMOUNT\", \"CREDIT_SCORE\", \"ACCOUNT_BALANCE\"],\n",
    "        output_cols=[\"AMOUNT_SCALED\", \"CREDIT_SCORE_SCALED\", \"ACCOUNT_BALANCE_SCALED\"]\n",
    "    )),\n",
    "    (\"Classifier\", RandomForestClassifier(\n",
    "        label_cols=[\"IS_FRAUD\"],\n",
    "        output_cols=[\"FRAUD_PREDICTION\"],\n",
    "        n_estimators=100,\n",
    "        max_depth=10\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "fraud_pipeline.fit(train_fraud)\n",
    "print(\"âœ… Fraud detection model trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Register Fraud Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "fraud_predictions = fraud_pipeline.predict(test_fraud)\n",
    "\n",
    "# Calculate metrics\n",
    "fraud_accuracy = accuracy_score(df=fraud_predictions, y_true_col_names=\"IS_FRAUD\", y_pred_col_names=\"FRAUD_PREDICTION\")\n",
    "fraud_metrics = {\"accuracy\": round(fraud_accuracy, 4)}\n",
    "print(f\"Fraud model metrics: {fraud_metrics}\")\n",
    "\n",
    "# Register model\n",
    "reg = Registry(session)\n",
    "fraud_version = reg.log_model(\n",
    "    model=fraud_pipeline,\n",
    "    model_name=\"FRAUD_DETECTION_MODEL\",\n",
    "    comment=\"Predicts transaction fraud using Random Forest based on transaction and customer features\",\n",
    "    metrics=fraud_metrics\n",
    ")\n",
    "\n",
    "print(f\"âœ… Fraud model registered as FRAUD_DETECTION_MODEL version {fraud_version.version_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODEL 2: Cash Advance Repayment Success\n",
    "\n",
    "Predict whether cash advances will be repaid successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Advance Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cash advance data with customer features\n",
    "advance_df = session.sql(\"\"\"\n",
    "SELECT\n",
    "    ca.advance_id,\n",
    "    ca.customer_id,\n",
    "    ca.advance_amount::FLOAT AS advance_amount,\n",
    "    ca.fee_amount::FLOAT AS fee_amount,\n",
    "    ca.eligibility_score::FLOAT AS eligibility_score,\n",
    "    c.credit_score::FLOAT AS credit_score,\n",
    "    c.risk_tier,\n",
    "    c.employment_status,\n",
    "    -- Count direct deposits\n",
    "    COUNT(DISTINCT dd.deposit_id)::FLOAT AS deposit_count,\n",
    "    -- Average deposit amount\n",
    "    AVG(dd.amount)::FLOAT AS avg_deposit_amount,\n",
    "    -- Target: Was repaid successfully\n",
    "    (ca.advance_status = 'REPAID')::BOOLEAN AS was_repaid\n",
    "FROM RAW.CASH_ADVANCES ca\n",
    "INNER JOIN RAW.CUSTOMERS c ON ca.customer_id = c.customer_id\n",
    "INNER JOIN RAW.DIRECT_DEPOSITS dd ON ca.customer_id = dd.customer_id\n",
    "WHERE ca.advance_date >= DATEADD('month', -12, CURRENT_DATE())\n",
    "  AND ca.eligibility_score IS NOT NULL\n",
    "  AND c.credit_score IS NOT NULL\n",
    "  AND c.risk_tier IS NOT NULL\n",
    "  AND c.employment_status IS NOT NULL\n",
    "  AND dd.amount IS NOT NULL\n",
    "GROUP BY ca.advance_id, ca.customer_id, ca.advance_amount, ca.fee_amount, ca.eligibility_score,\n",
    "         c.credit_score, c.risk_tier, c.employment_status, ca.advance_status\n",
    "HAVING AVG(dd.amount) IS NOT NULL\n",
    "  AND COUNT(DISTINCT dd.deposit_id) > 0\n",
    "LIMIT 5000\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Advance data: {advance_df.count()} advances\")\n",
    "advance_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Advance Repayment Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_advance, test_advance = advance_df.random_split([0.8, 0.2], seed=42)\n",
    "\n",
    "# Drop ID columns\n",
    "train_advance = train_advance.drop(\"ADVANCE_ID\", \"CUSTOMER_ID\")\n",
    "test_advance = test_advance.drop(\"ADVANCE_ID\", \"CUSTOMER_ID\")\n",
    "\n",
    "# Fill any remaining NaN/NULL values\n",
    "train_advance = train_advance.fillna({\n",
    "    \"ADVANCE_AMOUNT\": 100.0,\n",
    "    \"FEE_AMOUNT\": 5.0,\n",
    "    \"ELIGIBILITY_SCORE\": 0.5,\n",
    "    \"CREDIT_SCORE\": 650.0,\n",
    "    \"DEPOSIT_COUNT\": 0.0,\n",
    "    \"AVG_DEPOSIT_AMOUNT\": 1000.0\n",
    "})\n",
    "test_advance = test_advance.fillna({\n",
    "    \"ADVANCE_AMOUNT\": 100.0,\n",
    "    \"FEE_AMOUNT\": 5.0,\n",
    "    \"ELIGIBILITY_SCORE\": 0.5,\n",
    "    \"CREDIT_SCORE\": 650.0,\n",
    "    \"DEPOSIT_COUNT\": 0.0,\n",
    "    \"AVG_DEPOSIT_AMOUNT\": 1000.0\n",
    "})\n",
    "\n",
    "# Create pipeline\n",
    "advance_pipeline = Pipeline([\n",
    "    (\"Encoder\", OneHotEncoder(\n",
    "        input_cols=[\"RISK_TIER\", \"EMPLOYMENT_STATUS\"],\n",
    "        output_cols=[\"RISK_TIER_ENC\", \"EMPLOYMENT_STATUS_ENC\"],\n",
    "        drop_input_cols=True,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    (\"Scaler\", StandardScaler(\n",
    "        input_cols=[\"ADVANCE_AMOUNT\", \"FEE_AMOUNT\", \"ELIGIBILITY_SCORE\", \"CREDIT_SCORE\", \"DEPOSIT_COUNT\"],\n",
    "        output_cols=[\"ADVANCE_AMOUNT_SCALED\", \"FEE_AMOUNT_SCALED\", \"ELIGIBILITY_SCORE_SCALED\", \"CREDIT_SCORE_SCALED\", \"DEPOSIT_COUNT_SCALED\"]\n",
    "    )),\n",
    "    (\"Classifier\", LogisticRegression(\n",
    "        label_cols=[\"WAS_REPAID\"],\n",
    "        output_cols=[\"REPAYMENT_PREDICTION\"]\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "advance_pipeline.fit(train_advance)\n",
    "print(\"âœ… Advance repayment model trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Register Advance Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "advance_predictions = advance_pipeline.predict(test_advance)\n",
    "\n",
    "# Calculate metrics\n",
    "advance_accuracy = accuracy_score(df=advance_predictions, y_true_col_names=\"WAS_REPAID\", y_pred_col_names=\"REPAYMENT_PREDICTION\")\n",
    "advance_metrics = {\"accuracy\": round(advance_accuracy, 4)}\n",
    "print(f\"Advance model metrics: {advance_metrics}\")\n",
    "\n",
    "# Register model\n",
    "advance_version = reg.log_model(\n",
    "    model=advance_pipeline,\n",
    "    model_name=\"ADVANCE_ELIGIBILITY_MODEL\",\n",
    "    comment=\"Predicts cash advance repayment success using Logistic Regression based on customer creditworthiness and deposit patterns\",\n",
    "    metrics=advance_metrics\n",
    ")\n",
    "\n",
    "print(f\"âœ… Advance model registered as ADVANCE_ELIGIBILITY_MODEL version {advance_version.version_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODEL 3: Customer Lifetime Value Prediction\n",
    "\n",
    "Predict customer lifetime value using engagement and behavior metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LTV Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get customer LTV data with features\n",
    "ltv_df = session.sql(\"\"\"\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.lifetime_value::FLOAT AS lifetime_value,\n",
    "    DATEDIFF('month', c.acquisition_date, CURRENT_DATE())::FLOAT AS tenure_months,\n",
    "    c.credit_score::FLOAT AS credit_score,\n",
    "    c.risk_tier,\n",
    "    c.acquisition_channel,\n",
    "    -- Product count\n",
    "    COUNT(DISTINCT a.account_id)::FLOAT AS product_count,\n",
    "    -- Average account balance (handle NULL)\n",
    "    COALESCE(AVG(a.current_balance), 0)::FLOAT AS avg_account_balance,\n",
    "    -- Transaction count (last 90 days)\n",
    "    COUNT(DISTINCT CASE WHEN t.transaction_date >= DATEADD('day', -90, CURRENT_DATE())\n",
    "                   THEN t.transaction_id END)::FLOAT AS recent_transaction_count,\n",
    "    -- Has direct deposit\n",
    "    (COUNT(DISTINCT dd.deposit_id) > 0)::BOOLEAN AS has_direct_deposit\n",
    "FROM RAW.CUSTOMERS c\n",
    "LEFT JOIN RAW.ACCOUNTS a ON c.customer_id = a.customer_id\n",
    "LEFT JOIN RAW.TRANSACTIONS t ON c.customer_id = t.customer_id\n",
    "LEFT JOIN RAW.DIRECT_DEPOSITS dd ON c.customer_id = dd.customer_id\n",
    "WHERE c.customer_status = 'ACTIVE'\n",
    "  AND c.lifetime_value > 0\n",
    "GROUP BY c.customer_id, c.lifetime_value, c.acquisition_date, c.credit_score, c.risk_tier, c.acquisition_channel\n",
    "LIMIT 5000\n",
    "\"\"\")\n",
    "\n",
    "print(f\"LTV data: {ltv_df.count()} customers\")\n",
    "ltv_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LTV Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_ltv, test_ltv = ltv_df.random_split([0.8, 0.2], seed=42)\n",
    "\n",
    "# Drop CUSTOMER_ID\n",
    "train_ltv = train_ltv.drop(\"CUSTOMER_ID\")\n",
    "test_ltv = test_ltv.drop(\"CUSTOMER_ID\")\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "train_ltv = train_ltv.fillna({\n",
    "    \"TENURE_MONTHS\": 1.0,\n",
    "    \"CREDIT_SCORE\": 650.0,\n",
    "    \"PRODUCT_COUNT\": 1.0,\n",
    "    \"AVG_ACCOUNT_BALANCE\": 0.0,\n",
    "    \"RECENT_TRANSACTION_COUNT\": 0.0\n",
    "})\n",
    "test_ltv = test_ltv.fillna({\n",
    "    \"TENURE_MONTHS\": 1.0,\n",
    "    \"CREDIT_SCORE\": 650.0,\n",
    "    \"PRODUCT_COUNT\": 1.0,\n",
    "    \"AVG_ACCOUNT_BALANCE\": 0.0,\n",
    "    \"RECENT_TRANSACTION_COUNT\": 0.0\n",
    "})\n",
    "\n",
    "# Create pipeline\n",
    "ltv_pipeline = Pipeline([\n",
    "    (\"Encoder\", OneHotEncoder(\n",
    "        input_cols=[\"RISK_TIER\", \"ACQUISITION_CHANNEL\"],\n",
    "        output_cols=[\"RISK_TIER_ENC\", \"ACQUISITION_CHANNEL_ENC\"],\n",
    "        drop_input_cols=True,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    (\"Scaler\", StandardScaler(\n",
    "        input_cols=[\"TENURE_MONTHS\", \"CREDIT_SCORE\", \"PRODUCT_COUNT\", \"AVG_ACCOUNT_BALANCE\", \"RECENT_TRANSACTION_COUNT\"],\n",
    "        output_cols=[\"TENURE_MONTHS_SCALED\", \"CREDIT_SCORE_SCALED\", \"PRODUCT_COUNT_SCALED\", \"AVG_ACCOUNT_BALANCE_SCALED\", \"RECENT_TRANSACTION_COUNT_SCALED\"]\n",
    "    )),\n",
    "    (\"Regressor\", GradientBoostingRegressor(\n",
    "        label_cols=[\"LIFETIME_VALUE\"],\n",
    "        output_cols=[\"PREDICTED_LTV\"],\n",
    "        n_estimators=100,\n",
    "        max_depth=6\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "ltv_pipeline.fit(train_ltv)\n",
    "print(\"âœ… LTV prediction model trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Register LTV Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "ltv_predictions = ltv_pipeline.predict(test_ltv)\n",
    "\n",
    "# Calculate metrics\n",
    "ltv_mae = mean_absolute_error(df=ltv_predictions, y_true_col_names=\"LIFETIME_VALUE\", y_pred_col_names=\"PREDICTED_LTV\")\n",
    "ltv_rmse = mean_squared_error(df=ltv_predictions, y_true_col_names=\"LIFETIME_VALUE\", y_pred_col_names=\"PREDICTED_LTV\") ** 0.5\n",
    "ltv_metrics = {\"mae\": round(ltv_mae, 2), \"rmse\": round(ltv_rmse, 2)}\n",
    "print(f\"LTV model metrics: {ltv_metrics}\")\n",
    "\n",
    "# Register model\n",
    "ltv_version = reg.log_model(\n",
    "    model=ltv_pipeline,\n",
    "    model_name=\"CUSTOMER_LTV_MODEL\",\n",
    "    comment=\"Predicts customer lifetime value using Gradient Boosting based on engagement and behavior metrics\",\n",
    "    metrics=ltv_metrics\n",
    ")\n",
    "\n",
    "print(f\"âœ… LTV model registered as CUSTOMER_LTV_MODEL version {ltv_version.version_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Verify Models in Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all models in the registry\n",
    "print(\"Models in registry:\")\n",
    "reg.show_models()\n",
    "\n",
    "# Show versions for fraud model\n",
    "print(\"\\nFraud Detection Model versions:\")\n",
    "reg.get_model(\"FRAUD_DETECTION_MODEL\").show_versions()\n",
    "\n",
    "# Show versions for advance model  \n",
    "print(\"\\nAdvance Eligibility Model versions:\")\n",
    "reg.get_model(\"ADVANCE_ELIGIBILITY_MODEL\").show_versions()\n",
    "\n",
    "# Show versions for LTV model\n",
    "print(\"\\nCustomer LTV Model versions:\")\n",
    "reg.get_model(\"CUSTOMER_LTV_MODEL\").show_versions()\n",
    "\n",
    "print(\"\\nâœ… All models registered and ready to add to Intelligence Agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Model Inference\n",
    "\n",
    "Test calling each model to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fraud detection on sample transactions\n",
    "fraud_model = reg.get_model(\"FRAUD_DETECTION_MODEL\").default\n",
    "sample_fraud = fraud_df.limit(5).drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\")\n",
    "sample_fraud = sample_fraud.fillna({\"ACCOUNT_BALANCE\": 0, \"CREDIT_SCORE\": 650.0})\n",
    "fraud_preds = fraud_model.run(sample_fraud, function_name=\"predict\")\n",
    "print(\"Fraud Detection predictions:\")\n",
    "fraud_preds.select(\"IS_FRAUD\", \"FRAUD_PREDICTION\").show()\n",
    "\n",
    "# Test advance repayment on sample advances\n",
    "advance_model = reg.get_model(\"ADVANCE_ELIGIBILITY_MODEL\").default\n",
    "sample_advance = advance_df.limit(5).drop(\"ADVANCE_ID\", \"CUSTOMER_ID\")\n",
    "sample_advance = sample_advance.fillna({\n",
    "    \"ADVANCE_AMOUNT\": 100.0,\n",
    "    \"FEE_AMOUNT\": 5.0,\n",
    "    \"ELIGIBILITY_SCORE\": 0.5,\n",
    "    \"CREDIT_SCORE\": 650.0,\n",
    "    \"DEPOSIT_COUNT\": 0.0,\n",
    "    \"AVG_DEPOSIT_AMOUNT\": 1000.0\n",
    "})\n",
    "advance_preds = advance_model.run(sample_advance, function_name=\"predict\")\n",
    "print(\"\\nAdvance Repayment predictions:\")\n",
    "advance_preds.select(\"WAS_REPAID\", \"REPAYMENT_PREDICTION\").show()\n",
    "\n",
    "# Test LTV prediction on sample customers\n",
    "ltv_model = reg.get_model(\"CUSTOMER_LTV_MODEL\").default\n",
    "sample_ltv = ltv_df.limit(5).drop(\"CUSTOMER_ID\")\n",
    "sample_ltv = sample_ltv.fillna({\n",
    "    \"TENURE_MONTHS\": 1.0,\n",
    "    \"CREDIT_SCORE\": 650.0,\n",
    "    \"PRODUCT_COUNT\": 1.0,\n",
    "    \"AVG_ACCOUNT_BALANCE\": 0.0,\n",
    "    \"RECENT_TRANSACTION_COUNT\": 0.0\n",
    "})\n",
    "ltv_preds = ltv_model.run(sample_ltv, function_name=\"predict\")\n",
    "print(\"\\nCustomer LTV predictions:\")\n",
    "ltv_preds.select(\"LIFETIME_VALUE\", \"PREDICTED_LTV\").show()\n",
    "\n",
    "print(\"\\nâœ… All models tested successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Next Steps\n",
    "\n",
    "## Add Models to Intelligence Agent\n",
    "\n",
    "**Using the SQL Script (Recommended)**\n",
    "Run `sql/agent/10_create_intelligence_agent.sql` which automatically configures all ML model procedures.\n",
    "\n",
    "**The Python procedures** in `sql/ml/09_create_model_functions.sql` will use these registered models.\n",
    "\n",
    "## Example Questions for Agent\n",
    "\n",
    "- \"Is this $500 international transaction likely fraud?\"\n",
    "- \"Check if customer CUST00001234 is eligible for a cash advance\"\n",
    "- \"Predict the lifetime value for our newest customer cohort\"\n",
    "- \"Which customers show high fraud risk patterns?\"\n",
    "\n",
    "The models will now be available as tools your agent can use!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
