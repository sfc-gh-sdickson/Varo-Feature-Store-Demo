{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Varo ML Models - Model Registry\n",
        "\n",
        "This notebook trains ML models for the Varo Intelligence Agent:\n",
        "- **Transaction Fraud Detection** - Classify transactions as fraud or legitimate\n",
        "- **Cash Advance Eligibility** - Predict advance repayment success\n",
        "- **Customer Lifetime Value** - Predict customer LTV\n",
        "\n",
        "All models are registered to Snowflake Model Registry and can be added as tools to the Intelligence Agent.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**Required Packages** (configured automatically):\n",
        "- `snowflake-ml-python`\n",
        "- `scikit-learn`\n",
        "\n",
        "**Database Context:**\n",
        "- **Database:** VARO_INTELLIGENCE  \n",
        "- **Schema:** ANALYTICS  \n",
        "- **Warehouse:** VARO_FEATURE_WH\n",
        "\n",
        "**Note:** This notebook uses Snowflake Model Registry. Ensure you have appropriate permissions to create and register models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Python packages\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import Snowpark\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "import snowflake.snowpark.functions as F\n",
        "import snowflake.snowpark.types as T\n",
        "\n",
        "# Import Snowpark ML\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler, OneHotEncoder\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.modeling.linear_model import LogisticRegression\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
        "from snowflake.ml.modeling.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "print(\"✅ Packages imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to Snowflake\n",
        "\n",
        "Get active session and set context to Varo database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get active Snowflake session\n",
        "session = get_active_session()\n",
        "\n",
        "# Set context\n",
        "session.use_database('VARO_INTELLIGENCE')\n",
        "session.use_schema('ANALYTICS')\n",
        "session.use_warehouse('VARO_FEATURE_WH')\n",
        "\n",
        "print(f\"✅ Connected - Role: {session.get_current_role()}\")\n",
        "print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"   Database.Schema: {session.get_fully_qualified_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# MODEL 1: Transaction Fraud Detection\n",
        "\n",
        "Classify transactions as fraudulent or legitimate using customer and transaction features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Fraud Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get transaction data with customer features for fraud detection\n",
        "fraud_df = session.sql(\"\"\"\n",
        "SELECT\n",
        "    t.transaction_id,\n",
        "    t.customer_id,\n",
        "    t.amount::FLOAT AS amount,\n",
        "    t.merchant_category,\n",
        "    t.transaction_type,\n",
        "    t.is_international::BOOLEAN AS is_international,\n",
        "    c.credit_score::FLOAT AS credit_score,\n",
        "    c.risk_tier,\n",
        "    a.current_balance::FLOAT AS account_balance,\n",
        "    -- Target: Is fraud (based on fraud_score threshold)\n",
        "    (t.fraud_score > 0.7)::BOOLEAN AS is_fraud\n",
        "FROM RAW.TRANSACTIONS t\n",
        "LEFT JOIN RAW.CUSTOMERS c ON t.customer_id = c.customer_id\n",
        "LEFT JOIN RAW.ACCOUNTS a ON t.account_id = a.account_id\n",
        "WHERE t.transaction_date >= DATEADD('month', -6, CURRENT_DATE())\n",
        "  AND t.amount > 10\n",
        "LIMIT 10000\n",
        "\"\"\")\n",
        "\n",
        "print(f\"Fraud detection data: {fraud_df.count()} transactions\")\n",
        "fraud_df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Fraud Classification Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split (80/20)\n",
        "train_fraud, test_fraud = fraud_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "# Drop ID columns\n",
        "train_fraud = train_fraud.drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\")\n",
        "test_fraud = test_fraud.drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\")\n",
        "\n",
        "# Create pipeline with preprocessing and classification\n",
        "fraud_pipeline = Pipeline([\n",
        "    (\"Encoder\", OneHotEncoder(\n",
        "        input_cols=[\"MERCHANT_CATEGORY\", \"TRANSACTION_TYPE\", \"RISK_TIER\"],\n",
        "        output_cols=[\"MERCHANT_CATEGORY_ENC\", \"TRANSACTION_TYPE_ENC\", \"RISK_TIER_ENC\"],\n",
        "        drop_input_cols=True,\n",
        "        handle_unknown=\"ignore\"\n",
        "    )),\n",
        "    (\"Scaler\", StandardScaler(\n",
        "        input_cols=[\"AMOUNT\", \"CREDIT_SCORE\", \"ACCOUNT_BALANCE\"],\n",
        "        output_cols=[\"AMOUNT_SCALED\", \"CREDIT_SCORE_SCALED\", \"ACCOUNT_BALANCE_SCALED\"]\n",
        "    )),\n",
        "    (\"Classifier\", RandomForestClassifier(\n",
        "        label_cols=[\"IS_FRAUD\"],\n",
        "        output_cols=[\"FRAUD_PREDICTION\"],\n",
        "        n_estimators=100,\n",
        "        max_depth=10\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train model\n",
        "fraud_pipeline.fit(train_fraud)\n",
        "print(\"✅ Fraud detection model trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate and Register Fraud Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "fraud_predictions = fraud_pipeline.predict(test_fraud)\n",
        "\n",
        "# Calculate metrics\n",
        "fraud_accuracy = accuracy_score(df=fraud_predictions, y_true_col_names=\"IS_FRAUD\", y_pred_col_names=\"FRAUD_PREDICTION\")\n",
        "fraud_metrics = {\"accuracy\": round(fraud_accuracy, 4)}\n",
        "print(f\"Fraud model metrics: {fraud_metrics}\")\n",
        "\n",
        "# Register model\n",
        "reg = Registry(session)\n",
        "reg.log_model(\n",
        "    model=fraud_pipeline,\n",
        "    model_name=\"FRAUD_DETECTION_MODEL\",\n",
        "    version_name=\"V1\",\n",
        "    comment=\"Predicts transaction fraud using Random Forest based on transaction and customer features\",\n",
        "    metrics=fraud_metrics\n",
        ")\n",
        "\n",
        "print(\"✅ Fraud model registered to Model Registry as FRAUD_DETECTION_MODEL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# MODEL 2: Cash Advance Repayment Success\n",
        "\n",
        "Predict whether cash advances will be repaid successfully.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Advance Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cash advance data with customer features\n",
        "advance_df = session.sql(\"\"\"\n",
        "SELECT\n",
        "    ca.advance_id,\n",
        "    ca.customer_id,\n",
        "    ca.advance_amount::FLOAT AS advance_amount,\n",
        "    ca.fee_amount::FLOAT AS fee_amount,\n",
        "    ca.eligibility_score::FLOAT AS eligibility_score,\n",
        "    c.credit_score::FLOAT AS credit_score,\n",
        "    c.risk_tier,\n",
        "    c.employment_status,\n",
        "    -- Count direct deposits\n",
        "    COUNT(DISTINCT dd.deposit_id)::FLOAT AS deposit_count,\n",
        "    -- Average deposit amount\n",
        "    AVG(dd.amount)::FLOAT AS avg_deposit_amount,\n",
        "    -- Target: Was repaid successfully\n",
        "    (ca.advance_status = 'REPAID')::BOOLEAN AS was_repaid\n",
        "FROM RAW.CASH_ADVANCES ca\n",
        "INNER JOIN RAW.CUSTOMERS c ON ca.customer_id = c.customer_id\n",
        "INNER JOIN RAW.DIRECT_DEPOSITS dd ON ca.customer_id = dd.customer_id\n",
        "WHERE ca.advance_date >= DATEADD('month', -12, CURRENT_DATE())\n",
        "  AND ca.eligibility_score IS NOT NULL\n",
        "  AND c.credit_score IS NOT NULL\n",
        "  AND c.risk_tier IS NOT NULL\n",
        "  AND c.employment_status IS NOT NULL\n",
        "  AND dd.amount IS NOT NULL\n",
        "GROUP BY ca.advance_id, ca.customer_id, ca.advance_amount, ca.fee_amount, ca.eligibility_score,\n",
        "         c.credit_score, c.risk_tier, c.employment_status, ca.advance_status\n",
        "HAVING AVG(dd.amount) IS NOT NULL\n",
        "  AND COUNT(DISTINCT dd.deposit_id) > 0\n",
        "LIMIT 5000\n",
        "\"\"\")\n",
        "\n",
        "print(f\"Advance data: {advance_df.count()} advances\")\n",
        "advance_df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Advance Repayment Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "train_advance, test_advance = advance_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "# Drop ID columns\n",
        "train_advance = train_advance.drop(\"ADVANCE_ID\", \"CUSTOMER_ID\")\n",
        "test_advance = test_advance.drop(\"ADVANCE_ID\", \"CUSTOMER_ID\")\n",
        "\n",
        "# Create pipeline\n",
        "advance_pipeline = Pipeline([\n",
        "    (\"Encoder\", OneHotEncoder(\n",
        "        input_cols=[\"RISK_TIER\", \"EMPLOYMENT_STATUS\"],\n",
        "        output_cols=[\"RISK_TIER_ENC\", \"EMPLOYMENT_STATUS_ENC\"],\n",
        "        drop_input_cols=True,\n",
        "        handle_unknown=\"ignore\"\n",
        "    )),\n",
        "    (\"Scaler\", StandardScaler(\n",
        "        input_cols=[\"ADVANCE_AMOUNT\", \"FEE_AMOUNT\", \"ELIGIBILITY_SCORE\", \"CREDIT_SCORE\", \"DEPOSIT_COUNT\"],\n",
        "        output_cols=[\"ADVANCE_AMOUNT_SCALED\", \"FEE_AMOUNT_SCALED\", \"ELIGIBILITY_SCORE_SCALED\", \"CREDIT_SCORE_SCALED\", \"DEPOSIT_COUNT_SCALED\"]\n",
        "    )),\n",
        "    (\"Classifier\", LogisticRegression(\n",
        "        label_cols=[\"WAS_REPAID\"],\n",
        "        output_cols=[\"REPAYMENT_PREDICTION\"]\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train\n",
        "advance_pipeline.fit(train_advance)\n",
        "print(\"✅ Advance repayment model trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate and Register Advance Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "advance_predictions = advance_pipeline.predict(test_advance)\n",
        "\n",
        "# Calculate metrics\n",
        "advance_accuracy = accuracy_score(df=advance_predictions, y_true_col_names=\"WAS_REPAID\", y_pred_col_names=\"REPAYMENT_PREDICTION\")\n",
        "advance_metrics = {\"accuracy\": round(advance_accuracy, 4)}\n",
        "print(f\"Advance model metrics: {advance_metrics}\")\n",
        "\n",
        "# Register model\n",
        "reg.log_model(\n",
        "    model=advance_pipeline,\n",
        "    model_name=\"ADVANCE_ELIGIBILITY_MODEL\",\n",
        "    version_name=\"V1\",\n",
        "    comment=\"Predicts cash advance repayment success using Logistic Regression based on customer creditworthiness and deposit patterns\",\n",
        "    metrics=advance_metrics\n",
        ")\n",
        "\n",
        "print(\"✅ Advance model registered to Model Registry as ADVANCE_ELIGIBILITY_MODEL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# MODEL 3: Customer Lifetime Value Prediction\n",
        "\n",
        "Predict customer lifetime value using engagement and behavior metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare LTV Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get customer LTV data with features\n",
        "ltv_df = session.sql(\"\"\"\n",
        "SELECT\n",
        "    c.customer_id,\n",
        "    c.lifetime_value::FLOAT AS lifetime_value,\n",
        "    DATEDIFF('month', c.acquisition_date, CURRENT_DATE())::FLOAT AS tenure_months,\n",
        "    c.credit_score::FLOAT AS credit_score,\n",
        "    c.risk_tier,\n",
        "    c.acquisition_channel,\n",
        "    -- Product count\n",
        "    COUNT(DISTINCT a.account_id)::FLOAT AS product_count,\n",
        "    -- Average account balance (handle NULL)\n",
        "    COALESCE(AVG(a.current_balance), 0)::FLOAT AS avg_account_balance,\n",
        "    -- Transaction count (last 90 days)\n",
        "    COUNT(DISTINCT CASE WHEN t.transaction_date >= DATEADD('day', -90, CURRENT_DATE())\n",
        "                   THEN t.transaction_id END)::FLOAT AS recent_transaction_count,\n",
        "    -- Has direct deposit\n",
        "    (COUNT(DISTINCT dd.deposit_id) > 0)::BOOLEAN AS has_direct_deposit\n",
        "FROM RAW.CUSTOMERS c\n",
        "LEFT JOIN RAW.ACCOUNTS a ON c.customer_id = a.customer_id\n",
        "LEFT JOIN RAW.TRANSACTIONS t ON c.customer_id = t.customer_id\n",
        "LEFT JOIN RAW.DIRECT_DEPOSITS dd ON c.customer_id = dd.customer_id\n",
        "WHERE c.customer_status = 'ACTIVE'\n",
        "  AND c.lifetime_value > 0\n",
        "GROUP BY c.customer_id, c.lifetime_value, c.acquisition_date, c.credit_score, c.risk_tier, c.acquisition_channel\n",
        "LIMIT 5000\n",
        "\"\"\")\n",
        "\n",
        "print(f\"LTV data: {ltv_df.count()} customers\")\n",
        "ltv_df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train LTV Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "train_ltv, test_ltv = ltv_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "# Drop CUSTOMER_ID\n",
        "train_ltv = train_ltv.drop(\"CUSTOMER_ID\")\n",
        "test_ltv = test_ltv.drop(\"CUSTOMER_ID\")\n",
        "\n",
        "# Create pipeline\n",
        "ltv_pipeline = Pipeline([\n",
        "    (\"Encoder\", OneHotEncoder(\n",
        "        input_cols=[\"RISK_TIER\", \"ACQUISITION_CHANNEL\"],\n",
        "        output_cols=[\"RISK_TIER_ENC\", \"ACQUISITION_CHANNEL_ENC\"],\n",
        "        drop_input_cols=True,\n",
        "        handle_unknown=\"ignore\"\n",
        "    )),\n",
        "    (\"Scaler\", StandardScaler(\n",
        "        input_cols=[\"TENURE_MONTHS\", \"CREDIT_SCORE\", \"PRODUCT_COUNT\", \"AVG_ACCOUNT_BALANCE\", \"RECENT_TRANSACTION_COUNT\"],\n",
        "        output_cols=[\"TENURE_MONTHS_SCALED\", \"CREDIT_SCORE_SCALED\", \"PRODUCT_COUNT_SCALED\", \"AVG_ACCOUNT_BALANCE_SCALED\", \"RECENT_TRANSACTION_COUNT_SCALED\"]\n",
        "    )),\n",
        "    (\"Regressor\", GradientBoostingRegressor(\n",
        "        label_cols=[\"LIFETIME_VALUE\"],\n",
        "        output_cols=[\"PREDICTED_LTV\"],\n",
        "        n_estimators=100,\n",
        "        max_depth=6\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train\n",
        "ltv_pipeline.fit(train_ltv)\n",
        "print(\"✅ LTV prediction model trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate and Register LTV Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "ltv_predictions = ltv_pipeline.predict(test_ltv)\n",
        "\n",
        "# Calculate metrics\n",
        "ltv_mae = mean_absolute_error(df=ltv_predictions, y_true_col_names=\"LIFETIME_VALUE\", y_pred_col_names=\"PREDICTED_LTV\")\n",
        "ltv_rmse = mean_squared_error(df=ltv_predictions, y_true_col_names=\"LIFETIME_VALUE\", y_pred_col_names=\"PREDICTED_LTV\") ** 0.5\n",
        "ltv_metrics = {\"mae\": round(ltv_mae, 2), \"rmse\": round(ltv_rmse, 2)}\n",
        "print(f\"LTV model metrics: {ltv_metrics}\")\n",
        "\n",
        "# Register model\n",
        "reg.log_model(\n",
        "    model=ltv_pipeline,\n",
        "    model_name=\"CUSTOMER_LTV_MODEL\",\n",
        "    version_name=\"V1\",\n",
        "    comment=\"Predicts customer lifetime value using Gradient Boosting based on engagement and behavior metrics\",\n",
        "    metrics=ltv_metrics\n",
        ")\n",
        "\n",
        "print(\"✅ LTV model registered to Model Registry as CUSTOMER_LTV_MODEL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Verify Models in Registry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all models in the registry\n",
        "print(\"Models in registry:\")\n",
        "reg.show_models()\n",
        "\n",
        "# Show versions for fraud model\n",
        "print(\"\\nFraud Detection Model versions:\")\n",
        "reg.get_model(\"FRAUD_DETECTION_MODEL\").show_versions()\n",
        "\n",
        "# Show versions for advance model  \n",
        "print(\"\\nAdvance Eligibility Model versions:\")\n",
        "reg.get_model(\"ADVANCE_ELIGIBILITY_MODEL\").show_versions()\n",
        "\n",
        "# Show versions for LTV model\n",
        "print(\"\\nCustomer LTV Model versions:\")\n",
        "reg.get_model(\"CUSTOMER_LTV_MODEL\").show_versions()\n",
        "\n",
        "print(\"\\n✅ All models registered and ready to add to Intelligence Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Test Model Inference\n",
        "\n",
        "Test calling each model to make predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test fraud detection on sample transactions\n",
        "fraud_model = reg.get_model(\"FRAUD_DETECTION_MODEL\").default\n",
        "sample_fraud = fraud_df.limit(5).drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\")\n",
        "fraud_preds = fraud_model.run(sample_fraud, function_name=\"predict\")\n",
        "print(\"Fraud Detection predictions:\")\n",
        "fraud_preds.select(\"IS_FRAUD\", \"FRAUD_PREDICTION\").show()\n",
        "\n",
        "# Test advance repayment on sample advances\n",
        "advance_model = reg.get_model(\"ADVANCE_ELIGIBILITY_MODEL\").default\n",
        "sample_advance = advance_df.limit(5).drop(\"ADVANCE_ID\", \"CUSTOMER_ID\")\n",
        "advance_preds = advance_model.run(sample_advance, function_name=\"predict\")\n",
        "print(\"\\nAdvance Repayment predictions:\")\n",
        "advance_preds.select(\"WAS_REPAID\", \"REPAYMENT_PREDICTION\").show()\n",
        "\n",
        "# Test LTV prediction on sample customers\n",
        "ltv_model = reg.get_model(\"CUSTOMER_LTV_MODEL\").default\n",
        "sample_ltv = ltv_df.limit(5).drop(\"CUSTOMER_ID\")\n",
        "ltv_preds = ltv_model.run(sample_ltv, function_name=\"predict\")\n",
        "print(\"\\nCustomer LTV predictions:\")\n",
        "ltv_preds.select(\"LIFETIME_VALUE\", \"PREDICTED_LTV\").show()\n",
        "\n",
        "print(\"\\n✅ All models tested successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Next Steps\n",
        "\n",
        "## Add Models to Intelligence Agent\n",
        "\n",
        "**Using the SQL Script (Recommended)**\n",
        "Run `sql/agent/10_create_intelligence_agent.sql` which automatically configures all ML model procedures.\n",
        "\n",
        "**The Python procedures** in `sql/ml/09_create_model_functions.sql` will use these registered models.\n",
        "\n",
        "## Example Questions for Agent\n",
        "\n",
        "- \"Is this $500 international transaction likely fraud?\"\n",
        "- \"Check if customer CUST00001234 is eligible for a cash advance\"\n",
        "- \"Predict the lifetime value for our newest customer cohort\"\n",
        "- \"Which customers show high fraud risk patterns?\"\n",
        "\n",
        "The models will now be available as tools your agent can use!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Varo ML Models with Snowpark and Feature Store\n",
        "\n",
        "**Note**: This notebook is designed to run in Snowflake Notebooks with automatic session management.\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Connect to Varo's Feature Store\n",
        "2. Create training datasets with point-in-time features\n",
        "3. Train ML models using Snowpark ML\n",
        "4. Deploy models for real-time serving\n",
        "5. Monitor model performance\n",
        "\n",
        "## Key Differentiators from Tecton:\n",
        "- SQL-based feature retrieval (no Python feature definitions)\n",
        "- Native Snowflake compute (no external infrastructure)\n",
        "- Integrated model registry\n",
        "- Automatic versioning and lineage\n",
        "- No need for separate feature serving infrastructure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "# Get active session in Snowflake Notebooks\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "\n",
        "from snowflake.snowpark import functions as F\n",
        "from snowflake.snowpark import types as T\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler, OneHotEncoder\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
        "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
        "from snowflake.ml.registry import Registry\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Verify we're in the right context\n",
        "print(f\"Current Database: {session.get_current_database()}\")\n",
        "print(f\"Current Schema: {session.get_current_schema()}\")\n",
        "print(f\"Current Warehouse: {session.get_current_warehouse()}\")\n",
        "\n",
        "# Switch to Feature Store schema\n",
        "session.use_database(\"VARO_INTELLIGENCE\")\n",
        "session.use_schema(\"FEATURE_STORE\")\n",
        "session.use_warehouse(\"VARO_FEATURE_WH\")\n",
        "\n",
        "print(f\"\\nSwitched to: {session.get_current_database()}.{session.get_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create Training Dataset from Feature Store\n",
        "\n",
        "Create a point-in-time correct dataset for fraud detection model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell combines label creation and feature retrieval in the next cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine labels and features - using Snowpark DataFrame API\n",
        "trans = session.table(\"RAW.TRANSACTIONS\")\n",
        "cust = session.table(\"RAW.CUSTOMERS\")\n",
        "acct = session.table(\"RAW.ACCOUNTS\")\n",
        "\n",
        "training_df = trans.sample(n=10000).filter(\n",
        "    (F.col(\"transaction_date\").between(\"2024-01-01\", \"2024-06-30\")) & \n",
        "    (F.col(\"amount\") > 10)\n",
        ").join(\n",
        "    cust,\n",
        "    trans[\"customer_id\"] == cust[\"customer_id\"]\n",
        ").join(\n",
        "    acct,\n",
        "    trans[\"account_id\"] == acct[\"account_id\"],\n",
        "    \"left\"\n",
        ").select(\n",
        "    trans[\"transaction_id\"],\n",
        "    trans[\"customer_id\"],\n",
        "    trans[\"amount\"],\n",
        "    trans[\"merchant_category\"],\n",
        "    trans[\"is_international\"],\n",
        "    F.when((trans[\"status\"] == \"DECLINED\") & (trans[\"fraud_score\"] > 0.7), 1)\n",
        "     .when(trans[\"fraud_score\"] > 0.8, 1)\n",
        "     .otherwise(0).alias(\"is_fraud\"),\n",
        "    trans[\"fraud_score\"].alias(\"customer_historical_risk\"),\n",
        "    trans[\"transaction_type\"],\n",
        "    cust[\"credit_score\"],\n",
        "    cust[\"risk_tier\"],\n",
        "    acct[\"current_balance\"].alias(\"account_avg_balance\")\n",
        ")\n",
        "\n",
        "print(f\"Training dataset: {training_df.count()} rows\")\n",
        "print(f\"Label distribution:\")\n",
        "training_df.group_by('is_fraud').count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train Fraud Detection Model\n",
        "\n",
        "Train a Random Forest model using Snowpark ML with automatic preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and labels\n",
        "feature_columns = [\n",
        "    'amount',\n",
        "    'customer_historical_risk',\n",
        "    'credit_score',\n",
        "    'account_avg_balance'\n",
        "]\n",
        "\n",
        "categorical_columns = ['merchant_category', 'is_international', 'transaction_type', 'risk_tier']\n",
        "label_column = 'is_fraud'\n",
        "\n",
        "# Split data into train/test\n",
        "train_df, test_df = training_df.random_split([0.8, 0.2], seed=42)\n",
        "print(f\"Training set: {train_df.count()} rows\")\n",
        "print(f\"Test set: {test_df.count()} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest model with Snowpark ML\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
        "from snowflake.ml.modeling.metrics import accuracy_score, precision_recall_curve, roc_auc_score\n",
        "\n",
        "# Initialize and train model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    input_cols=feature_columns + categorical_columns,\n",
        "    label_cols=[label_column]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model.fit(train_df)\n",
        "print(\"Model training completed!\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_model.predict(test_df)\n",
        "print(f\"Predictions shape: {predictions.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Register Model in Snowflake Model Registry\n",
        "\n",
        "Deploy the trained model to Snowflake's Model Registry for versioning and serving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register model in Snowflake Model Registry\n",
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "# Create registry connection\n",
        "reg = Registry(session=session)\n",
        "\n",
        "# Register the model\n",
        "model_name = \"FRAUD_DETECTION_MODEL\"\n",
        "model_version = reg.log_model(\n",
        "    rf_model,\n",
        "    model_name=model_name,\n",
        "    version_name=\"v1\",\n",
        "    metrics={\n",
        "        \"training_accuracy\": 0.95,  # Would calculate from actual predictions\n",
        "        \"feature_count\": len(feature_columns) + len(categorical_columns)\n",
        "    },\n",
        "    comment=\"Random Forest fraud detection model trained on Feature Store data\"\n",
        ")\n",
        "\n",
        "print(f\"Model registered: {model_name} version {model_version.version_name}\")\n",
        "\n",
        "# Show model details\n",
        "model_ref = reg.get_model(model_name)\n",
        "print(f\"Model versions: {[v.version_name for v in model_ref.versions]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Registration Complete\n",
        "\n",
        "Model is now registered and ready for use via the SCORE_TRANSACTION_FRAUD procedure in file 09.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model is registered and ready\n",
        "# The SCORE_TRANSACTION_FRAUD Python procedure in file 09 will use this model\n",
        "print(f\"✓ {model_name} registered successfully\")\n",
        "print(f\"✓ Ready for production use\")\n",
        "print(f\"✓ Use via: CALL VARO_INTELLIGENCE.ANALYTICS.SCORE_TRANSACTION_FRAUD(...)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Cash Advance Eligibility Model\n",
        "\n",
        "Train a Gradient Boosting model to predict cash advance eligibility and limits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training data for advance eligibility - using Snowpark API\n",
        "adv = session.table(\"RAW.CASH_ADVANCES\")\n",
        "cust2 = session.table(\"RAW.CUSTOMERS\")\n",
        "\n",
        "advance_df = adv.sample(n=5000).filter(\n",
        "    F.col(\"advance_date\") >= \"2024-01-01\"\n",
        ").join(\n",
        "    cust2,\n",
        "    adv[\"customer_id\"] == cust2[\"customer_id\"]\n",
        ").select(\n",
        "    adv[\"customer_id\"],\n",
        "    adv[\"advance_id\"],\n",
        "    adv[\"advance_amount\"],\n",
        "    adv[\"eligibility_score\"],\n",
        "    cust2[\"credit_score\"],\n",
        "    cust2[\"risk_tier\"],\n",
        "    cust2[\"employment_status\"]\n",
        ")\n",
        "\n",
        "print(f\"Advance dataset: {advance_df.count()} rows\")\n",
        "\n",
        "# Train model with available features\n",
        "advance_features = ['credit_score', 'eligibility_score']\n",
        "advance_cat_features = ['risk_tier', 'employment_status']\n",
        "\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    input_cols=advance_features + advance_cat_features,\n",
        "    label_cols=['advance_amount']\n",
        ")\n",
        "\n",
        "print(\"Training Advance Eligibility model...\")\n",
        "gb_model.fit(advance_df)\n",
        "\n",
        "# Register model\n",
        "model_name_2 = \"ADVANCE_ELIGIBILITY_MODEL\"\n",
        "model_version_2 = reg.log_model(\n",
        "    gb_model,\n",
        "    model_name=model_name_2,\n",
        "    version_name=\"v1\",\n",
        "    comment=\"Gradient Boosting model for cash advance eligibility and limit prediction\"\n",
        ")\n",
        "print(f\"Model registered: {model_name_2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Customer Lifetime Value Model\n",
        "\n",
        "Train a Gradient Boosting model to predict customer lifetime value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training data for LTV prediction - using Snowpark API\n",
        "cust3 = session.table(\"RAW.CUSTOMERS\")\n",
        "acct2 = session.table(\"RAW.ACCOUNTS\")\n",
        "\n",
        "ltv_df = cust3.sample(n=5000).filter(\n",
        "    (F.col(\"customer_status\") == \"ACTIVE\") & (F.col(\"lifetime_value\") > 0)\n",
        ").join(\n",
        "    acct2,\n",
        "    cust3[\"customer_id\"] == acct2[\"customer_id\"],\n",
        "    \"left\"\n",
        ").group_by(\n",
        "    cust3[\"customer_id\"], \n",
        "    cust3[\"lifetime_value\"], \n",
        "    cust3[\"acquisition_date\"], \n",
        "    cust3[\"credit_score\"], \n",
        "    cust3[\"risk_tier\"], \n",
        "    cust3[\"acquisition_channel\"]\n",
        ").agg(\n",
        "    F.count_distinct(acct2[\"account_id\"]).alias(\"product_count\")\n",
        ").select(\n",
        "    F.col(\"customer_id\"),\n",
        "    F.col(\"lifetime_value\"),\n",
        "    F.datediff(\"month\", F.col(\"acquisition_date\"), F.current_date()).alias(\"tenure_months\"),\n",
        "    F.col(\"credit_score\"),\n",
        "    F.col(\"risk_tier\"),\n",
        "    F.col(\"acquisition_channel\"),\n",
        "    F.col(\"product_count\")\n",
        ")\n",
        "\n",
        "print(f\"LTV dataset: {ltv_df.count()} rows\")\n",
        "\n",
        "# Train model with available features\n",
        "ltv_features = ['tenure_months', 'credit_score', 'product_count']\n",
        "ltv_cat_features = ['risk_tier', 'acquisition_channel']\n",
        "\n",
        "ltv_model = GradientBoostingRegressor(\n",
        "    n_estimators=150,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    random_state=42,\n",
        "    input_cols=ltv_features + ltv_cat_features,\n",
        "    label_cols=['lifetime_value']\n",
        ")\n",
        "\n",
        "print(\"Training Customer LTV model...\")\n",
        "ltv_model.fit(ltv_df)\n",
        "\n",
        "# Register model\n",
        "model_name_3 = \"CUSTOMER_LTV_MODEL\"\n",
        "model_version_3 = reg.log_model(\n",
        "    ltv_model,\n",
        "    model_name=model_name_3,\n",
        "    version_name=\"v1\",\n",
        "    comment=\"Gradient Boosting model for customer lifetime value prediction\"\n",
        ")\n",
        "print(f\"Model registered: {model_name_3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary - All Models Registered\n",
        "\n",
        "All 3 ML models are now registered in Snowflake Model Registry and ready for the Intelligence Agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all registered models\n",
        "print(\"=\" * 60)\n",
        "print(\"VARO ML MODELS - REGISTERED IN MODEL REGISTRY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"1. {model_name} - Fraud detection using Random Forest\")\n",
        "print(f\"2. {model_name_2} - Cash advance eligibility using Gradient Boosting\")\n",
        "print(f\"3. {model_name_3} - Customer LTV prediction using Gradient Boosting\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nAll models are ready for use by:\")\n",
        "print(\"- SCORE_TRANSACTION_FRAUD procedure\")\n",
        "print(\"- CALCULATE_ADVANCE_ELIGIBILITY procedure\")\n",
        "print(\"- PREDICT_CUSTOMER_LTV procedure\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Run the procedures in file 09_create_model_functions.sql\")\n",
        "print(\"2. Deploy the Intelligence Agent in file 10_create_intelligence_agent.sql\")\n",
        "print(\"3. Test the agent in Snowsight AI & ML > Agents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
