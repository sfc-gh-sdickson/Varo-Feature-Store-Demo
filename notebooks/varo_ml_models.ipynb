{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varo ML Models - Feature Store with Online Serving & Model Registry\n",
    "\n",
    "This notebook implements a complete Snowflake Feature Store with **online feature serving** for real-time inference, followed by ML model training.\n",
    "\n",
    "## Features\n",
    "- **Online Feature Serving**: Sub-second latency for real-time inference\n",
    "- **4 Feature Views**: CUSTOMER_PROFILE, TRANSACTION_PATTERN, ADVANCE_RISK, FRAUD_DETECTION\n",
    "- **3 ML Models**: Fraud Detection, Advance Eligibility, Customer LTV\n",
    "- **Incremental Refresh**: Efficient data sync using change tracking\n",
    "\n",
    "## Prerequisites\n",
    "- Snowflake v9.26+ and snowflake-ml-python v1.18.0+\n",
    "- Database: VARO_INTELLIGENCE\n",
    "- Warehouse: VARO_FEATURE_WH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Snowpark\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "# Feature Store with Online Config\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode,\n",
    "    StoreType\n",
    ")\n",
    "from snowflake.ml.feature_store.feature_view import OnlineConfig\n",
    "\n",
    "# ML Pipeline & Registry\n",
    "from snowflake.ml.modeling.preprocessing import StandardScaler, OneHotEncoder\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.linear_model import LogisticRegression\n",
    "from snowflake.ml.modeling.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from snowflake.ml.modeling.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "print(\"âœ… All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "session.use_database('VARO_INTELLIGENCE')\n",
    "session.use_schema('ANALYTICS')\n",
    "session.use_warehouse('VARO_FEATURE_WH')\n",
    "\n",
    "print(f\"âœ… Connected to Snowflake\")\n",
    "print(f\"   Role: {session.get_current_role()}\")\n",
    "print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"   Context: {session.get_fully_qualified_current_schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Feature Store & Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Feature Store\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=\"VARO_INTELLIGENCE\",\n",
    "    name=\"FEATURE_STORE\",\n",
    "    default_warehouse=\"VARO_FEATURE_WH\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "# Register CUSTOMER entity\n",
    "customer_entity = Entity(\n",
    "    name=\"CUSTOMER\",\n",
    "    join_keys=[\"customer_id\"],\n",
    "    desc=\"Varo bank customer\"\n",
    ")\n",
    "fs.register_entity(customer_entity)\n",
    "\n",
    "print(\"âœ… Feature Store connected\")\n",
    "print(\"âœ… CUSTOMER entity registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-Time Setup: Enable Change Tracking & Clean Previous Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create aggregation views (required for Feature Views)\n",
    "aggregation_views_sql = \"\"\"\n",
    "-- Customer Profile Aggregations\n",
    "CREATE OR REPLACE VIEW VARO_INTELLIGENCE.FEATURE_STORE.V_CUSTOMER_PROFILE_AGGS AS\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    COUNT(DISTINCT a.account_id) as num_accounts\n",
    "FROM VARO_INTELLIGENCE.RAW.CUSTOMERS c\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.ACCOUNTS a ON c.customer_id = a.customer_id\n",
    "GROUP BY c.customer_id;\n",
    "\n",
    "-- Transaction Pattern Aggregations\n",
    "CREATE OR REPLACE VIEW VARO_INTELLIGENCE.FEATURE_STORE.V_TRANSACTION_PATTERN_AGGS AS\n",
    "SELECT\n",
    "    customer_id,\n",
    "    COUNT(CASE WHEN transaction_date >= DATEADD('day', -7, CURRENT_DATE()) THEN 1 END) as txn_count_7d,\n",
    "    SUM(CASE WHEN transaction_date >= DATEADD('day', -7, CURRENT_DATE()) THEN ABS(amount) END) as txn_volume_7d,\n",
    "    COUNT(CASE WHEN transaction_timestamp >= DATEADD('hour', -1, CURRENT_TIMESTAMP()) THEN 1 END) as txn_count_1h\n",
    "FROM VARO_INTELLIGENCE.RAW.TRANSACTIONS\n",
    "WHERE status = 'COMPLETED'\n",
    "GROUP BY customer_id;\n",
    "\n",
    "-- Advance Risk Aggregations\n",
    "CREATE OR REPLACE VIEW VARO_INTELLIGENCE.FEATURE_STORE.V_ADVANCE_RISK_AGGS AS\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    COUNT(ca.advance_id) as total_advances_taken,\n",
    "    AVG(ca.advance_amount) as avg_advance_amount,\n",
    "    COUNT(CASE WHEN ca.advance_status = 'DEFAULTED' THEN 1 END) as num_defaults\n",
    "FROM VARO_INTELLIGENCE.RAW.CUSTOMERS c\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.CASH_ADVANCES ca ON c.customer_id = ca.customer_id\n",
    "GROUP BY c.customer_id;\n",
    "\n",
    "-- Fraud Detection Aggregations\n",
    "CREATE OR REPLACE VIEW VARO_INTELLIGENCE.FEATURE_STORE.V_FRAUD_DETECTION_AGGS AS\n",
    "WITH recent_transactions AS (\n",
    "    SELECT \n",
    "        t.*,\n",
    "        LAG(transaction_timestamp) OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) as prev_txn_time,\n",
    "        LAG(merchant_city) OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) as prev_merchant_city\n",
    "    FROM VARO_INTELLIGENCE.RAW.TRANSACTIONS t\n",
    "    WHERE transaction_timestamp >= DATEADD('day', -7, CURRENT_TIMESTAMP()) AND status = 'COMPLETED'\n",
    ")\n",
    "SELECT\n",
    "    t.customer_id,\n",
    "    MAX(CASE WHEN ABS(t.amount) > avg_txn.avg_amount * 3 AND ABS(t.amount) > 100 THEN 1 ELSE 0 END) as has_unusual_amount,\n",
    "    MAX(CASE WHEN DATEDIFF('minute', t.prev_txn_time, t.transaction_timestamp) < 5 AND t.merchant_city != t.prev_merchant_city THEN 1 ELSE 0 END) as impossible_travel_flag\n",
    "FROM recent_transactions t\n",
    "LEFT JOIN (\n",
    "    SELECT customer_id, AVG(ABS(amount)) as avg_amount\n",
    "    FROM VARO_INTELLIGENCE.RAW.TRANSACTIONS\n",
    "    WHERE transaction_date >= DATEADD('day', -30, CURRENT_DATE())\n",
    "    GROUP BY customer_id\n",
    ") avg_txn ON t.customer_id = avg_txn.customer_id\n",
    "GROUP BY t.customer_id;\n",
    "\"\"\"\n",
    "\n",
    "for command in aggregation_views_sql.split(';'):\n",
    "    if command.strip():\n",
    "        try:\n",
    "            session.sql(command).collect()\n",
    "        except Exception as e:\n",
    "            if \"already exists\" not in str(e):\n",
    "                raise e\n",
    "print(\"âœ… Aggregation views created\")\n",
    "\n",
    "# Step 2: Clean up any corrupted objects from previous failed registrations\n",
    "view_names = [\n",
    "    '\"CUSTOMER_PROFILE_FEATURES$v1\"',\n",
    "    '\"TRANSACTION_PATTERN_FEATURES$v1\"',\n",
    "    '\"ADVANCE_RISK_FEATURES$v1\"',\n",
    "    '\"FRAUD_DETECTION_FEATURES$v1\"'\n",
    "]\n",
    "\n",
    "for view_name in view_names:\n",
    "    try:\n",
    "        session.sql(f'DROP DYNAMIC TABLE IF EXISTS VARO_INTELLIGENCE.FEATURE_STORE.{view_name}').collect()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"âœ… Cleaned up previous registration attempts\")\n",
    "print(\"\\nðŸ”§ Setup complete - ready to register Feature Views\")\n",
    "print(\"   Note: Using FULL refresh mode (no change tracking required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Online Feature Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time serving: 30-second freshness (for fraud detection)\n",
    "online_config_realtime = OnlineConfig(\n",
    "    enable=True,\n",
    "    target_lag=\"30 seconds\"\n",
    ")\n",
    "\n",
    "# Hourly serving: 5-minute freshness (for profile features)\n",
    "online_config_hourly = OnlineConfig(\n",
    "    enable=True,\n",
    "    target_lag=\"5 minutes\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Online configs defined\")\n",
    "print(\"   - Realtime: 30 seconds\")\n",
    "print(\"   - Hourly: 5 minutes\")\n",
    "\n",
    "# CRITICAL: Switch to FEATURE_STORE schema before registering Feature Views\n",
    "# This ensures the online table creation can find the offline Dynamic Table\n",
    "session.use_schema('FEATURE_STORE')\n",
    "print(\"âœ… Switched to FEATURE_STORE schema for registration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Register Feature Views with Online Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CUSTOMER_PROFILE_FEATURES - Query ONLY the source table, no JOINs\n",
    "customer_profile_df = session.sql(\"\"\"\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        updated_at as feature_timestamp,\n",
    "        date_of_birth,\n",
    "        acquisition_date,\n",
    "        credit_score,\n",
    "        income_verified\n",
    "    FROM VARO_INTELLIGENCE.RAW.CUSTOMERS\n",
    "    WHERE customer_status = 'ACTIVE'\n",
    "\"\"\")\n",
    "\n",
    "customer_profile_fv = FeatureView(\n",
    "    name=\"CUSTOMER_PROFILE_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=customer_profile_df,\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    refresh_freq=\"1 hour\",\n",
    "    refresh_mode=\"FULL\",\n",
    "    desc=\"Customer demographic, account, and profile features\",\n",
    "    online_config=online_config_hourly\n",
    ")\n",
    "fs.register_feature_view(feature_view=customer_profile_fv, version=\"v1\", block=True)\n",
    "print(\"âœ… CUSTOMER_PROFILE_FEATURES registered with online serving (5min lag)\")\n",
    "\n",
    "# 2. TRANSACTION_PATTERN_FEATURES\n",
    "transaction_pattern_df = session.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        t.customer_id,\n",
    "        t.created_at as feature_timestamp,\n",
    "        COALESCE(v.txn_count_7d, 0) as txn_count_7d,\n",
    "        COALESCE(v.txn_volume_7d, 0) as txn_volume_7d,\n",
    "        COALESCE(v.txn_count_1h, 0) as txn_count_1h\n",
    "    FROM VARO_INTELLIGENCE.RAW.TRANSACTIONS t\n",
    "    LEFT JOIN VARO_INTELLIGENCE.FEATURE_STORE.V_TRANSACTION_PATTERN_AGGS v ON t.customer_id = v.customer_id\n",
    "    WHERE t.status = 'COMPLETED'\n",
    "\"\"\")\n",
    "\n",
    "transaction_pattern_fv = FeatureView(\n",
    "    name=\"TRANSACTION_PATTERN_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=transaction_pattern_df,\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    refresh_freq=\"30 minutes\",\n",
    "    refresh_mode=\"FULL\",\n",
    "    desc=\"Transaction patterns, velocity, and spending behavior\",\n",
    "    online_config=online_config_realtime\n",
    ")\n",
    "fs.register_feature_view(feature_view=transaction_pattern_fv, version=\"v1\", block=True)\n",
    "print(\"âœ… TRANSACTION_PATTERN_FEATURES registered with online serving (30sec lag)\")\n",
    "\n",
    "# 3. ADVANCE_RISK_FEATURES\n",
    "advance_risk_df = session.sql(\"\"\"\n",
    "    SELECT\n",
    "        c.customer_id,\n",
    "        c.updated_at as feature_timestamp,\n",
    "        COALESCE(v.total_advances_taken, 0) as total_advances_taken,\n",
    "        COALESCE(v.avg_advance_amount, 0) as avg_advance_amount,\n",
    "        COALESCE(v.num_defaults, 0) as num_defaults\n",
    "    FROM VARO_INTELLIGENCE.RAW.CUSTOMERS c\n",
    "    LEFT JOIN VARO_INTELLIGENCE.FEATURE_STORE.V_ADVANCE_RISK_AGGS v ON c.customer_id = v.customer_id\n",
    "    WHERE c.customer_status = 'ACTIVE'\n",
    "\"\"\")\n",
    "\n",
    "advance_risk_fv = FeatureView(\n",
    "    name=\"ADVANCE_RISK_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=advance_risk_df,\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    refresh_freq=\"1 hour\",\n",
    "    refresh_mode=\"FULL\",\n",
    "    desc=\"Cash advance history and risk indicators\",\n",
    "    online_config=online_config_hourly\n",
    ")\n",
    "fs.register_feature_view(feature_view=advance_risk_fv, version=\"v1\", block=True)\n",
    "print(\"âœ… ADVANCE_RISK_FEATURES registered with online serving (5min lag)\")\n",
    "\n",
    "# 4. FRAUD_DETECTION_FEATURES\n",
    "fraud_detection_df = session.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        t.customer_id,\n",
    "        t.created_at as feature_timestamp,\n",
    "        COALESCE(v.has_unusual_amount, 0) as has_unusual_amount,\n",
    "        COALESCE(v.impossible_travel_flag, 0) as impossible_travel_flag\n",
    "    FROM VARO_INTELLIGENCE.RAW.TRANSACTIONS t\n",
    "    LEFT JOIN VARO_INTELLIGENCE.FEATURE_STORE.V_FRAUD_DETECTION_AGGS v ON t.customer_id = v.customer_id\n",
    "    WHERE t.status = 'COMPLETED'\n",
    "\"\"\")\n",
    "\n",
    "fraud_detection_fv = FeatureView(\n",
    "    name=\"FRAUD_DETECTION_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=fraud_detection_df,\n",
    "    timestamp_col=\"feature_timestamp\",\n",
    "    refresh_freq=\"15 minutes\",\n",
    "    refresh_mode=\"FULL\",\n",
    "    desc=\"Real-time fraud indicators and anomalies\",\n",
    "    online_config=online_config_realtime\n",
    ")\n",
    "fs.register_feature_view(feature_view=fraud_detection_fv, version=\"v1\", block=True)\n",
    "print(\"âœ… FRAUD_DETECTION_FEATURES registered with online serving (30sec lag)\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All Feature Views registered with online serving enabled!\")\n",
    "print(\"   Access via Snowflake UI: AI/ML > Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demonstrate Online Feature Retrieval (Low-Latency Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample customer ID\n",
    "sample_cust = session.sql(\"SELECT customer_id FROM VARO_INTELLIGENCE.RAW.CUSTOMERS LIMIT 1\").collect()\n",
    "if sample_cust:\n",
    "    customer_id = sample_cust[0][0]\n",
    "    \n",
    "    # Retrieve features from ONLINE store (sub-second latency)\n",
    "    try:\n",
    "        online_features = fs.read_feature_view(\n",
    "            feature_view=fraud_detection_fv,\n",
    "            keys=[[customer_id]],\n",
    "            store_type=StoreType.ONLINE\n",
    "        )\n",
    "        print(f\"âœ… Retrieved online features for customer {customer_id}\")\n",
    "        print(f\"   Latency: Sub-second from online store\")\n",
    "        print(online_features.show())\n",
    "    except Exception as e:\n",
    "        print(f\"â„¹ï¸  Online store still warming up: {e}\")\n",
    "        print(\"   This is normal on first run - data syncs in background\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No customers found - ensure data is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ML MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 1: Transaction Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fraud training data\n",
    "fraud_df = session.sql(\"\"\"\n",
    "SELECT\n",
    "    t.transaction_id,\n",
    "    t.customer_id,\n",
    "    t.amount::FLOAT AS amount,\n",
    "    COALESCE(t.merchant_category, 'UNKNOWN') as merchant_category,\n",
    "    COALESCE(t.transaction_type, 'DEBIT') as transaction_type,\n",
    "    COALESCE(t.is_international, FALSE)::BOOLEAN AS is_international,\n",
    "    COALESCE(c.credit_score, 650)::FLOAT AS credit_score,\n",
    "    COALESCE(c.risk_tier, 'MEDIUM') as risk_tier,\n",
    "    COALESCE(a.current_balance, 0)::FLOAT AS account_balance,\n",
    "    (COALESCE(t.fraud_score, 0) > 0.7)::BOOLEAN AS is_fraud\n",
    "FROM VARO_INTELLIGENCE.RAW.TRANSACTIONS t\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.CUSTOMERS c ON t.customer_id = c.customer_id\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.ACCOUNTS a ON t.account_id = a.account_id\n",
    "WHERE t.transaction_date >= DATEADD('month', -6, CURRENT_DATE())\n",
    "  AND t.amount > 10\n",
    "LIMIT 10000\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Fraud dataset: {fraud_df.count()} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and prepare\n",
    "train_fraud, test_fraud = fraud_df.random_split([0.8, 0.2], seed=42)\n",
    "train_fraud = train_fraud.drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\").fillna({\"ACCOUNT_BALANCE\": 0, \"CREDIT_SCORE\": 650.0})\n",
    "test_fraud = test_fraud.drop(\"TRANSACTION_ID\", \"CUSTOMER_ID\").fillna({\"ACCOUNT_BALANCE\": 0, \"CREDIT_SCORE\": 650.0})\n",
    "\n",
    "# Pipeline\n",
    "fraud_pipeline = Pipeline([\n",
    "    (\"Encoder\", OneHotEncoder(\n",
    "        input_cols=[\"MERCHANT_CATEGORY\", \"TRANSACTION_TYPE\", \"RISK_TIER\"],\n",
    "        output_cols=[\"MERCHANT_CATEGORY_ENC\", \"TRANSACTION_TYPE_ENC\", \"RISK_TIER_ENC\"],\n",
    "        drop_input_cols=True,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    (\"Scaler\", StandardScaler(\n",
    "        input_cols=[\"AMOUNT\", \"CREDIT_SCORE\", \"ACCOUNT_BALANCE\"],\n",
    "        output_cols=[\"AMOUNT_SCALED\", \"CREDIT_SCORE_SCALED\", \"ACCOUNT_BALANCE_SCALED\"]\n",
    "    )),\n",
    "    (\"Classifier\", RandomForestClassifier(\n",
    "        label_cols=[\"IS_FRAUD\"],\n",
    "        output_cols=[\"FRAUD_PREDICTION\"],\n",
    "        n_estimators=100,\n",
    "        max_depth=10\n",
    "    ))\n",
    "])\n",
    "\n",
    "fraud_pipeline.fit(train_fraud)\n",
    "print(\"âœ… Fraud model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "fraud_predictions = fraud_pipeline.predict(test_fraud)\n",
    "fraud_accuracy = accuracy_score(df=fraud_predictions, y_true_col_names=\"IS_FRAUD\", y_pred_col_names=\"FRAUD_PREDICTION\")\n",
    "\n",
    "# Register\n",
    "reg = Registry(session)\n",
    "fraud_version = reg.log_model(\n",
    "    model=fraud_pipeline,\n",
    "    model_name=\"FRAUD_DETECTION_MODEL\",\n",
    "    comment=\"Random Forest fraud classifier\",\n",
    "    metrics={\"accuracy\": round(fraud_accuracy, 4)}\n",
    ")\n",
    "\n",
    "print(f\"âœ… FRAUD_DETECTION_MODEL registered - Accuracy: {fraud_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model 2: Cash Advance Repayment Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load advance training data\n",
    "advance_df = session.sql(\"\"\"\n",
    "SELECT\n",
    "    ca.advance_id,\n",
    "    ca.customer_id,\n",
    "    ca.advance_amount::FLOAT AS advance_amount,\n",
    "    ca.fee_amount::FLOAT AS fee_amount,\n",
    "    ca.eligibility_score::FLOAT AS eligibility_score,\n",
    "    c.credit_score::FLOAT AS credit_score,\n",
    "    c.risk_tier,\n",
    "    c.employment_status,\n",
    "    COUNT(DISTINCT dd.deposit_id)::FLOAT AS deposit_count,\n",
    "    AVG(dd.amount)::FLOAT AS avg_deposit_amount,\n",
    "    (ca.advance_status = 'REPAID')::BOOLEAN AS was_repaid\n",
    "FROM VARO_INTELLIGENCE.RAW.CASH_ADVANCES ca\n",
    "INNER JOIN VARO_INTELLIGENCE.RAW.CUSTOMERS c ON ca.customer_id = c.customer_id\n",
    "INNER JOIN VARO_INTELLIGENCE.RAW.DIRECT_DEPOSITS dd ON ca.customer_id = dd.customer_id\n",
    "WHERE ca.advance_date >= DATEADD('month', -12, CURRENT_DATE())\n",
    "  AND ca.eligibility_score IS NOT NULL\n",
    "  AND c.credit_score IS NOT NULL\n",
    "  AND c.risk_tier IS NOT NULL\n",
    "  AND c.employment_status IS NOT NULL\n",
    "  AND dd.amount IS NOT NULL\n",
    "GROUP BY ca.advance_id, ca.customer_id, ca.advance_amount, ca.fee_amount, ca.eligibility_score,\n",
    "         c.credit_score, c.risk_tier, c.employment_status, ca.advance_status\n",
    "HAVING AVG(dd.amount) IS NOT NULL AND COUNT(DISTINCT dd.deposit_id) > 0\n",
    "LIMIT 5000\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Advance dataset: {advance_df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and prepare\n",
    "train_adv, test_adv = advance_df.random_split([0.8, 0.2], seed=42)\n",
    "train_adv = train_adv.drop(\"ADVANCE_ID\", \"CUSTOMER_ID\").fillna({\n",
    "    \"ADVANCE_AMOUNT\": 100.0, \"FEE_AMOUNT\": 5.0, \"ELIGIBILITY_SCORE\": 0.5,\n",
    "    \"CREDIT_SCORE\": 650.0, \"DEPOSIT_COUNT\": 0.0, \"AVG_DEPOSIT_AMOUNT\": 1000.0\n",
    "})\n",
    "test_adv = test_adv.drop(\"ADVANCE_ID\", \"CUSTOMER_ID\").fillna({\n",
    "    \"ADVANCE_AMOUNT\": 100.0, \"FEE_AMOUNT\": 5.0, \"ELIGIBILITY_SCORE\": 0.5,\n",
    "    \"CREDIT_SCORE\": 650.0, \"DEPOSIT_COUNT\": 0.0, \"AVG_DEPOSIT_AMOUNT\": 1000.0\n",
    "})\n",
    "\n",
    "# Pipeline\n",
    "advance_pipeline = Pipeline([\n",
    "    (\"Encoder\", OneHotEncoder(\n",
    "        input_cols=[\"RISK_TIER\", \"EMPLOYMENT_STATUS\"],\n",
    "        output_cols=[\"RISK_TIER_ENC\", \"EMPLOYMENT_STATUS_ENC\"],\n",
    "        drop_input_cols=True,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    (\"Scaler\", StandardScaler(\n",
    "        input_cols=[\"ADVANCE_AMOUNT\", \"FEE_AMOUNT\", \"ELIGIBILITY_SCORE\", \"CREDIT_SCORE\", \"DEPOSIT_COUNT\"],\n",
    "        output_cols=[\"ADVANCE_AMOUNT_SCALED\", \"FEE_AMOUNT_SCALED\", \"ELIGIBILITY_SCORE_SCALED\", \"CREDIT_SCORE_SCALED\", \"DEPOSIT_COUNT_SCALED\"]\n",
    "    )),\n",
    "    (\"Classifier\", LogisticRegression(\n",
    "        label_cols=[\"WAS_REPAID\"],\n",
    "        output_cols=[\"REPAYMENT_PREDICTION\"]\n",
    "    ))\n",
    "])\n",
    "\n",
    "advance_pipeline.fit(train_adv)\n",
    "print(\"âœ… Advance model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "advance_predictions = advance_pipeline.predict(test_adv)\n",
    "advance_accuracy = accuracy_score(df=advance_predictions, y_true_col_names=\"WAS_REPAID\", y_pred_col_names=\"REPAYMENT_PREDICTION\")\n",
    "\n",
    "# Register\n",
    "advance_version = reg.log_model(\n",
    "    model=advance_pipeline,\n",
    "    model_name=\"ADVANCE_ELIGIBILITY_MODEL\",\n",
    "    comment=\"Logistic Regression for advance repayment prediction\",\n",
    "    metrics={\"accuracy\": round(advance_accuracy, 4)}\n",
    ")\n",
    "\n",
    "print(f\"âœ… ADVANCE_ELIGIBILITY_MODEL registered - Accuracy: {advance_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model 3: Customer Lifetime Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LTV training data\n",
    "ltv_df = session.sql(\"\"\"\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.lifetime_value::FLOAT AS lifetime_value,\n",
    "    DATEDIFF('month', c.acquisition_date, CURRENT_DATE())::FLOAT AS tenure_months,\n",
    "    c.credit_score::FLOAT AS credit_score,\n",
    "    c.risk_tier,\n",
    "    c.acquisition_channel,\n",
    "    COUNT(DISTINCT a.account_id)::FLOAT AS product_count,\n",
    "    COALESCE(AVG(a.current_balance), 0)::FLOAT AS avg_account_balance,\n",
    "    COUNT(DISTINCT CASE WHEN t.transaction_date >= DATEADD('day', -90, CURRENT_DATE())\n",
    "                   THEN t.transaction_id END)::FLOAT AS recent_transaction_count,\n",
    "    (COUNT(DISTINCT dd.deposit_id) > 0)::BOOLEAN AS has_direct_deposit\n",
    "FROM VARO_INTELLIGENCE.RAW.CUSTOMERS c\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.ACCOUNTS a ON c.customer_id = a.customer_id\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.TRANSACTIONS t ON c.customer_id = t.customer_id\n",
    "LEFT JOIN VARO_INTELLIGENCE.RAW.DIRECT_DEPOSITS dd ON c.customer_id = dd.customer_id\n",
    "WHERE c.customer_status = 'ACTIVE' AND c.lifetime_value > 0\n",
    "GROUP BY c.customer_id, c.lifetime_value, c.acquisition_date, c.credit_score, c.risk_tier, c.acquisition_channel\n",
    "LIMIT 5000\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… LTV dataset: {ltv_df.count()} customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and prepare\n",
    "train_ltv, test_ltv = ltv_df.random_split([0.8, 0.2], seed=42)\n",
    "train_ltv = train_ltv.drop(\"CUSTOMER_ID\").fillna({\n",
    "    \"TENURE_MONTHS\": 1.0, \"CREDIT_SCORE\": 650.0, \"PRODUCT_COUNT\": 1.0,\n",
    "    \"AVG_ACCOUNT_BALANCE\": 0.0, \"RECENT_TRANSACTION_COUNT\": 0.0\n",
    "})\n",
    "test_ltv = test_ltv.drop(\"CUSTOMER_ID\").fillna({\n",
    "    \"TENURE_MONTHS\": 1.0, \"CREDIT_SCORE\": 650.0, \"PRODUCT_COUNT\": 1.0,\n",
    "    \"AVG_ACCOUNT_BALANCE\": 0.0, \"RECENT_TRANSACTION_COUNT\": 0.0\n",
    "})\n",
    "\n",
    "# Pipeline\n",
    "ltv_pipeline = Pipeline([\n",
    "    (\"Encoder\", OneHotEncoder(\n",
    "        input_cols=[\"RISK_TIER\", \"ACQUISITION_CHANNEL\"],\n",
    "        output_cols=[\"RISK_TIER_ENC\", \"ACQUISITION_CHANNEL_ENC\"],\n",
    "        drop_input_cols=True,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    (\"Scaler\", StandardScaler(\n",
    "        input_cols=[\"TENURE_MONTHS\", \"CREDIT_SCORE\", \"PRODUCT_COUNT\", \"AVG_ACCOUNT_BALANCE\", \"RECENT_TRANSACTION_COUNT\"],\n",
    "        output_cols=[\"TENURE_MONTHS_SCALED\", \"CREDIT_SCORE_SCALED\", \"PRODUCT_COUNT_SCALED\", \"AVG_ACCOUNT_BALANCE_SCALED\", \"RECENT_TRANSACTION_COUNT_SCALED\"]\n",
    "    )),\n",
    "    (\"Regressor\", GradientBoostingRegressor(\n",
    "        label_cols=[\"LIFETIME_VALUE\"],\n",
    "        output_cols=[\"PREDICTED_LTV\"],\n",
    "        n_estimators=100,\n",
    "        max_depth=6\n",
    "    ))\n",
    "])\n",
    "\n",
    "ltv_pipeline.fit(train_ltv)\n",
    "print(\"âœ… LTV model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "ltv_predictions = ltv_pipeline.predict(test_ltv)\n",
    "ltv_mae = mean_absolute_error(df=ltv_predictions, y_true_col_names=\"LIFETIME_VALUE\", y_pred_col_names=\"PREDICTED_LTV\")\n",
    "ltv_rmse = mean_squared_error(df=ltv_predictions, y_true_col_names=\"LIFETIME_VALUE\", y_pred_col_names=\"PREDICTED_LTV\") ** 0.5\n",
    "\n",
    "# Register\n",
    "ltv_version = reg.log_model(\n",
    "    model=ltv_pipeline,\n",
    "    model_name=\"CUSTOMER_LTV_MODEL\",\n",
    "    comment=\"Gradient Boosting for LTV prediction\",\n",
    "    metrics={\"mae\": round(ltv_mae, 2), \"rmse\": round(ltv_rmse, 2)}\n",
    ")\n",
    "\n",
    "print(f\"âœ… CUSTOMER_LTV_MODEL registered - MAE: {ltv_mae:.2f}, RMSE: {ltv_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary: All Models & Feature Views Registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ DEPLOYMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š FEATURE VIEWS (with Online Serving):\")\n",
    "print(\"  âœ… CUSTOMER_PROFILE_FEATURES (5-minute freshness)\")\n",
    "print(\"  âœ… TRANSACTION_PATTERN_FEATURES (30-second freshness)\")\n",
    "print(\"  âœ… ADVANCE_RISK_FEATURES (5-minute freshness)\")\n",
    "print(\"  âœ… FRAUD_DETECTION_FEATURES (30-second freshness)\")\n",
    "\n",
    "print(\"\\nðŸ¤– ML MODELS (in Registry):\")\n",
    "print(\"  âœ… FRAUD_DETECTION_MODEL\")\n",
    "print(\"  âœ… ADVANCE_ELIGIBILITY_MODEL\")\n",
    "print(\"  âœ… CUSTOMER_LTV_MODEL\")\n",
    "\n",
    "print(\"\\nðŸš€ NEXT STEPS:\")\n",
    "print(\"  1. Features visible in Snowflake UI: AI/ML > Features\")\n",
    "print(\"  2. Models visible in Snowflake UI: AI/ML > Models\")\n",
    "print(\"  3. Online serving enables sub-second latency inference\")\n",
    "print(\"  4. Run sql/agent/10_create_intelligence_agent.sql to add to agent\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
